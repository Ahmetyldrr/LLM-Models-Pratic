
# ğŸš€ BERT-TabanlÄ± Encoder-Decoder NLP Projesi â€“ UÃ§tan Uca Rehber

Bu projede, BERT tabanlÄ± bir encoder-decoder mimarisi kullanÄ±larak bir **metin Ã¶zetleme** sistemi kurulacaktÄ±r. Proje; veri hazÄ±rlama, model eÄŸitimi, deÄŸerlendirme ve daÄŸÄ±tÄ±mÄ± (deployment) adÄ±mlarÄ±nÄ± iÃ§erir.

---

## ğŸ“ 1. Ã–rnek Veri KÃ¼mesi

Proje iÃ§in kullanabileceÄŸimiz Ã¶rnek veri kÃ¼mesi **CNN/DailyMail dataset** olabilir. Bu veri seti haber metinlerini ve onlarÄ±n kÄ±sa Ã¶zetlerini iÃ§erir.

### ğŸ“Š Ã–rnek Format:

| Article | Summary |
|---------|---------|
| "The stock market crashed today due to..." | "Stock market crash." |
| "Scientists discovered a new planet..." | "New planet found." |

Alternatif olarak basit bir Ã¶rnek CSV oluÅŸturabilirsin:

```python
import pandas as pd

data = {
    "article": [
        "The Eiffel Tower is located in Paris and was built in 1889.",
        "The Amazon rainforest is home to diverse wildlife and vegetation.",
    ],
    "summary": [
        "Eiffel Tower is in Paris.",
        "Amazon rainforest has diverse wildlife."
    ]
}

df = pd.DataFrame(data)
df.to_csv("summary_dataset.csv", index=False)
```

---

## âš™ï¸ 2. Proje OrtamÄ± ve Gereksinimler

```bash
pip install transformers datasets torch gradio
```

---

## ğŸ§  3. Model EÄŸitimi (Training)

Burada `encoder-decoder` modeli olarak Hugging Face'ten `bert2bert` kullanÄ±labilir:

```python
from transformers import EncoderDecoderModel, BertTokenizer, Trainer, TrainingArguments, Seq2SeqTrainer
from datasets import load_dataset, Dataset
import torch

# Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Veri YÃ¼kle
import pandas as pd
dataset = pd.read_csv("summary_dataset.csv")

# Tokenization fonksiyonu
def preprocess_function(examples):
    inputs = tokenizer(examples["article"], padding="max_length", truncation=True, max_length=256)
    targets = tokenizer(examples["summary"], padding="max_length", truncation=True, max_length=64)
    inputs["labels"] = targets["input_ids"]
    return inputs

# Dataset formatÄ±
ds = Dataset.from_pandas(dataset)
tokenized_ds = ds.map(preprocess_function, batched=True)

# Model yÃ¼kle
model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "bert-base-uncased")

# Training args
training_args = TrainingArguments(
    output_dir="./bert2bert-summary",
    per_device_train_batch_size=2,
    num_train_epochs=5,
    logging_steps=10,
    save_steps=50,
    save_total_limit=2,
    evaluation_strategy="no",
)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds,
    tokenizer=tokenizer
)

trainer.train()
```

---

## âœ… 4. Model Testi (Inference)

```python
def generate_summary(text):
    inputs = tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=256)
    output = model.generate(**inputs, max_length=64)
    return tokenizer.decode(output[0], skip_special_tokens=True)

print(generate_summary("The Amazon rainforest is home to many species..."))
```

---

## ğŸŒ 5. Deployment (Gradio ile Web ArayÃ¼z)

```python
import gradio as gr

interface = gr.Interface(
    fn=generate_summary,
    inputs="text",
    outputs="text",
    title="BERT2BERT Summarizer",
    description="Makale giriÅŸinden Ã¶zet Ã¼retir."
)

interface.launch()
```

---

## ğŸ› ï¸ 6. Alternatif Deployment YÃ¶ntemleri

| YÃ¶ntem | AÃ§Ä±klama |
|--------|----------|
| `Flask` / `FastAPI` | API olarak sunmak iÃ§in ideal |
| `Docker` | Her yerde aynÄ± ortamda Ã§alÄ±ÅŸtÄ±rmak iÃ§in |
| `Hugging Face Spaces` | Gradio + Web sunumunu barÄ±ndÄ±rmak iÃ§in |
| `Streamlit` | GÃ¶rselleÅŸtirme de gerekiyorsa kullanÄ±labilir |

---

## ğŸ”š SonuÃ§

Bu projede:
- BERT tabanlÄ± encoder-decoder kullanarak Ã¶zetleme modeli eÄŸitildi
- Basit bir veri seti ile Ã§alÄ±ÅŸÄ±ldÄ±
- Model Gradio ile servis edildi

GerÃ§ek veri setleri ile performans artÄ±ÅŸÄ± saÄŸlanabilir. Bu yapÄ±yÄ± soru-cevap, Ã§eviri gibi seq2seq gÃ¶revlerde de kullanabilirsin.

